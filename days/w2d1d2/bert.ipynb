{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum, nn\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n",
      "torch.Size([2, 3, 768]) torch.Size([2, 3, 768])\n",
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.002978 STD: 0.1198 VALS [0.0555 -0.01219 0.02629 -0.06629 0.02201 0.06769 -0.01915 -0.02644 0.1231 -0.0728...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):   \n",
    "    print(token_activations.shape)\n",
    "    queries = project_query(token_activations)\n",
    "    keys = project_key(token_activations)\n",
    "    print(queries.shape, keys.shape)\n",
    "    queries = rearrange(queries, \"b n (h c) -> b h n c\", h=num_heads)\n",
    "    keys = rearrange(keys, \"b n (h c) -> b h n c\", h=num_heads)\n",
    "    result = einsum(\"bhnc,bhmc->bhnm\", keys, queries)\n",
    "    return result / math.sqrt(queries.shape[-1])\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001244 STD: 0.1123 VALS [0.02294 0.04522 0.1502 -0.04545 -0.08154 -0.1219 -0.01069 -0.03793 -0.1267 0.09468...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    softmax_fn = torch.nn.Softmax(dim=-2)\n",
    "    attention_pattern = softmax_fn(attention_pattern)\n",
    "    values = project_value(token_activations)\n",
    "    values = rearrange(values, \"b n (h c) -> b h n c\", h=num_heads)\n",
    "    output = einsum(\"bhkq, bhkc -> bhqc\", attention_pattern, values)\n",
    "    output = rearrange(output, \"b h n c -> b n (h c)\")\n",
    "    result = project_output(output)\n",
    "    return result\n",
    "\n",
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n",
      "torch.Size([2, 3, 768]) torch.Size([2, 3, 768])\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query_embedding = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_embedding = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_embedding = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_embedding = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, token_activations):\n",
    "        raw_attention = raw_attention_pattern(token_activations, self.num_heads, self.query_embedding, self.key_embedding)\n",
    "        return bert_attention(token_activations, self.num_heads, raw_attention, self.value_embedding, self.output_embedding)\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0003685 STD: 0.1087 VALS [-0.07879 -0.1109 0.1264 -0.173 -0.06065 0.1507 -0.03468 -0.2432 -0.09689 0.05654...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    return linear_2(nn.GELU()(linear_1(token_activations)))\n",
    "bert_tests.test_bert_mlp(bert_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, intermediate_size)\n",
    "        self.linear2 = nn.Linear(intermediate_size, input_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return bert_mlp(X, self.linear1, self.linear2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10]) torch.Size([10])\n",
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: 2.384e-09 STD: 1.003 VALS [-0.5207 0.05752 2.278 0.05417 -1.073 -0.2957 -1.152 -0.5042 1.302 -0.1456...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim):\n",
    "        super().__init__()\n",
    "        self.weight = torch.ones(normalized_dim)\n",
    "        self.bias = torch.zeros(normalized_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        std = torch.std(X.detach(), -1, unbiased=False)\n",
    "        mean = torch.mean(X.detach(), -1)\n",
    "\n",
    "        print(((X.detach() - mean.unsqueeze(-1)) / std.unsqueeze(-1)).shape, self.weight.shape)\n",
    "        return ((X.detach() - mean.unsqueeze(-1)) / std.unsqueeze(-1)) * self.weight + self.bias\n",
    "\n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n",
      "torch.Size([2, 3, 768]) torch.Size([2, 3, 768])\n",
      "torch.Size([2, 3, 768]) torch.Size([768])\n",
      "torch.Size([2, 3, 768]) torch.Size([768])\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -4.139e-09 STD: 1 VALS [0.007131 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "\n",
    "        # attention: batches tokens heads * channels -> batches tokens heads * channels\n",
    "        # layer_norm: batches tokens heads * channels -> batches tokens heads * channels\n",
    "        # mlp: batches tokens heads * channels -> batches tokens heads * channels\n",
    "        # layer_norm: batches tokens heads * channels\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.mlp = BertMLP(hidden_size, intermediate_size) # hidden_size = heads * channels\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        residual1 = torch.clone(X)\n",
    "        output1 = self.layer_norm(self.attention(X) + residual1)\n",
    "        residual2 = torch.clone(output1)\n",
    "        return self.layer_norm(self.dropout(self.mlp(output1)) + residual2)\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 117, 146, 1821, 170, 5650, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer(['Hello, I am a sentence.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.4328 STD: 1.09 VALS [-1.529 -1.113 1.017 -0.9385 -1.151 -0.8435 0.0199 -0.7648 1.023 -1.396...]\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed_matrix = torch.nn.Parameter(torch.randn(vocab_size, embed_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return (self.embed_matrix[X, :]).to(X.device)\n",
    "\n",
    "bert_tests.test_embedding(Embedding)\n",
    "\n",
    "x = torch.randint(0, 10, (2, 3)).cuda()\n",
    "print(x.is_cuda)\n",
    "emb1 = Embedding(10, 5)\n",
    "print(emb1(x).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embedding(\n",
    "    input_ids,\n",
    "    token_type_ids,\n",
    "    position_embedding,\n",
    "    token_embedding,\n",
    "    token_type_embedding,\n",
    "    layer_norm\n",
    "):\n",
    "    pass\n",
    "    \n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_embed = Embedding(vocab_size, dropout)\n",
    "        self.position_embed = torch.nn.Parameter(torch.randn(max_position_embeddings, hidden_size))\n",
    "        self.token_type_embed = torch.nn.Parameter(torch.randn(type_vocab_size, hidden_size))\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(\n",
    "            input_ids, \n",
    "            token_type_ids, \n",
    "            self.position_embed, \n",
    "            self.vocab_embed,\n",
    "            self.token_type_embed,\n",
    "            self.layer_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}